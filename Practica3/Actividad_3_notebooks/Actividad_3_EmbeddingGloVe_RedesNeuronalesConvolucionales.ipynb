{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktFg2jj3jTE6"
      },
      "source": [
        "# ACTIVIDAD DE CLASIFICACIÓN DE TEXTO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ-OuW5DiLJs"
      },
      "source": [
        "En esta actividad vamos a trabajar en clasificar textos. Se recorrerá todo el proceso desde traer el dataset hasta proceder a dicha clasificación. Durante la actividad se llevarán a cabo muchos procesos como la creación de un vocabulario, el uso de embeddings y la creación de modelos.\n",
        "\n",
        "Las cuestiones presentes en esta actividad están basadas en un Notebook creado por François Chollet, uno de los creadores de Keras y autor del libro \"Deep Learning with Python\". \n",
        "\n",
        "En este Notebook se trabaja con el dataset \"Newsgroup20\" que contiene aproximadamente 20000 mensajes que pertenecen a 20 categorías diferentes.\n",
        "\n",
        "El objetivo es entender los conceptos que se trabajan y ser capaz de hacer pequeñas experimentaciones para mejorar el Notebook creado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hytURWLLjZvT"
      },
      "source": [
        "#Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DbxRuvOwkzSs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXfYbCflkQYy"
      },
      "source": [
        "# Descarga de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e-1ZhOf3lB_A"
      },
      "outputs": [],
      "source": [
        "data_path = keras.utils.get_file(\n",
        "    \"news20.tar.gz\",\n",
        "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
        "    untar=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3ygvoWhlCYj",
        "outputId": "d5fbae1d-dc02-4069-abf2-a23e51325ed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of directories: 20\n",
            "Directory names: ['comp.windows.x', 'rec.sport.hockey', 'alt.atheism', 'sci.space', 'talk.politics.misc', 'sci.crypt', 'comp.sys.ibm.pc.hardware', 'rec.sport.baseball', 'sci.med', 'sci.electronics', 'talk.religion.misc', 'comp.graphics', 'comp.os.ms-windows.misc', 'rec.autos', 'talk.politics.mideast', 'comp.sys.mac.hardware', 'soc.religion.christian', 'talk.politics.guns', 'rec.motorcycles', 'misc.forsale']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "#Estructura de directorios del dataset\n",
        "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
        "dirnames = os.listdir(data_dir)\n",
        "print(\"Number of directories:\", len(dirnames))\n",
        "print(\"Directory names:\", dirnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG8rjgOFlcaV",
        "outputId": "74798fd3-86f4-4437-8714-ba7ca2a7dbd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in comp.graphics: 1000\n",
            "Some example filenames: ['38341', '38512', '38619', '38821', '38864']\n"
          ]
        }
      ],
      "source": [
        "#Algunos archivos de la categoria \"com.graphics\"\n",
        "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
        "print(\"Number of files in comp.graphics:\", len(fnames))\n",
        "print(\"Some example filenames:\", fnames[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ox6s6z9lgps",
        "outputId": "81d5f397-fadc-43b6-9c18-bd8c82eb6d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path: cantaloupe.srv.cs.cmu.edu!rochester!udel!gatech!howland.reston.ans.net!newsserver.jvnc.net!castor.hahnemann.edu!hal.hahnemann.edu!brennan\n",
            "From: brennan@hal.hahnemann.edu\n",
            "Newsgroups: comp.graphics\n",
            "Subject: .GIFs on a Tek401x ??\n",
            "Date: 15 MAY 93 14:29:54 EST\n",
            "Organization: Hahnemann University\n",
            "Lines: 14\n",
            "Message-ID: <15MAY93.14295461@hal.hahnemann.edu>\n",
            "NNTP-Posting-Host: hal.hahnemann.edu\n",
            "\n",
            "\n",
            "      I was skimming through a few gophers and bumped into one at NIH\n",
            "   with a database that included images in .GIF format.  While I have\n",
            "   not yet worked out the kinks of getting the gopher client to call\n",
            "   an X viewer, I figure that the majority of the users here are not\n",
            "   in an X11 environment - instead using DOS and MS-Kermit.\n",
            "\n",
            "      With Kermit supporting Tek4010 emulation for graphics display,\n",
            "   does anyone know of a package that would allow a Tek to display a\n",
            "   .GIF image?  It would be of more use to the local population to\n",
            "   plug something of this sort in as the 'picture' command instead of\n",
            "   XView or XLoadImage ...\n",
            "\n",
            "      andrew.  (brennan@hal.hahnemann.edu)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Ejemplo de un texto de la categoría \"com.graphics\"\n",
        "print(open(data_dir / \"comp.graphics\" / \"39625\").read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUbbjI8plaG0",
        "outputId": "4b2907dc-e9c4-4953-9815-5bb117ed5cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in talk.politics.misc: 1000\n",
            "Some example filenames: ['178736', '178951', '178528', '178388', '178677']\n"
          ]
        }
      ],
      "source": [
        "#Algunos archivos de la categoria \"talk.politics.misc\"\n",
        "fnames = os.listdir(data_dir / \"talk.politics.misc\")\n",
        "print(\"Number of files in talk.politics.misc:\", len(fnames))\n",
        "print(\"Some example filenames:\", fnames[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izZGWhpklCbI",
        "outputId": "dd6589f2-37b5-47ff-c3a4-a98fa6d36c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Xref: cantaloupe.srv.cs.cmu.edu talk.politics.guns:54219 talk.politics.misc:178463\n",
            "Newsgroups: talk.politics.guns,talk.politics.misc\n",
            "Path: cantaloupe.srv.cs.cmu.edu!magnesium.club.cc.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!magnus.acs.ohio-state.edu!usenet.ins.cwru.edu!agate!spool.mu.edu!darwin.sura.net!martha.utcc.utk.edu!FRANKENSTEIN.CE.UTK.EDU!VEAL\n",
            "From: VEAL@utkvm1.utk.edu (David Veal)\n",
            "Subject: Re: Proof of the Viability of Gun Control\n",
            "Message-ID: <VEAL.749.735192116@utkvm1.utk.edu>\n",
            "Lines: 21\n",
            "Sender: usenet@martha.utcc.utk.edu (USENET News System)\n",
            "Organization: University of Tennessee Division of Continuing Education\n",
            "References: <1qpbqd$ntl@access.digex.net> <C5otvp.ItL@magpie.linknet.com>\n",
            "Date: Mon, 19 Apr 1993 04:01:56 GMT\n",
            "\n",
            "[alt.drugs and alt.conspiracy removed from newsgroups line.]\n",
            "\n",
            "In article <C5otvp.ItL@magpie.linknet.com> neal@magpie.linknet.com (Neal) writes:\n",
            "\n",
            ">   Once the National Guard has been called into federal service,\n",
            ">it is under the command of the present. Tha National Guard, though\n",
            ">defined as the \"Militia\" in the statutes, is actually a reserve component\n",
            ">of the United State Army, and was formed pursuant to the power of Congress\n",
            ">to raise and support Armies.\n",
            "\n",
            "       That's the really cute thing about saying the 2nd amendment\n",
            "only covers the national guard, because that would mean that it\n",
            "essentially prohibits the federal government from disarming a branch\n",
            "of the federal government.\n",
            "\n",
            "       Sounds like a real limit to federal power to me.\n",
            "------------------------------------------------------------------------\n",
            "David Veal Univ. of Tenn. Div. of Cont. Education Info. Services Group\n",
            "PA146008@utkvm1.utk.edu - \"I still remember the way you laughed, the day\n",
            "your pushed me down the elevator shaft;  I'm beginning to think you don't\n",
            "love me anymore.\" - \"Weird Al\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Ejemplo de un texto de la categoría \"talk.politics.misc\"\n",
        "print(open(data_dir / \"talk.politics.misc\" / \"178463\").read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33Ay5U6blCd1",
        "outputId": "78413a5e-36ab-43b7-d164-703f1d3594ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing alt.atheism, 1000 files found\n",
            "Processing comp.graphics, 1000 files found\n",
            "Processing comp.os.ms-windows.misc, 1000 files found\n",
            "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
            "Processing comp.sys.mac.hardware, 1000 files found\n",
            "Processing comp.windows.x, 1000 files found\n",
            "Processing misc.forsale, 1000 files found\n",
            "Processing rec.autos, 1000 files found\n",
            "Processing rec.motorcycles, 1000 files found\n",
            "Processing rec.sport.baseball, 1000 files found\n",
            "Processing rec.sport.hockey, 1000 files found\n",
            "Processing sci.crypt, 1000 files found\n",
            "Processing sci.electronics, 1000 files found\n",
            "Processing sci.med, 1000 files found\n",
            "Processing sci.space, 1000 files found\n",
            "Processing soc.religion.christian, 997 files found\n",
            "Processing talk.politics.guns, 1000 files found\n",
            "Processing talk.politics.mideast, 1000 files found\n",
            "Processing talk.politics.misc, 1000 files found\n",
            "Processing talk.religion.misc, 1000 files found\n",
            "Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "Number of samples: 19997\n"
          ]
        }
      ],
      "source": [
        "samples = []\n",
        "labels = []\n",
        "class_names = []\n",
        "class_index = 0\n",
        "for dirname in sorted(os.listdir(data_dir)):\n",
        "    class_names.append(dirname)\n",
        "    dirpath = data_dir / dirname\n",
        "    fnames = os.listdir(dirpath)\n",
        "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
        "    for fname in fnames:\n",
        "        fpath = dirpath / fname\n",
        "        f = open(fpath, encoding=\"latin-1\")\n",
        "        content = f.read()\n",
        "        lines = content.split(\"\\n\")\n",
        "        lines = lines[10:]\n",
        "        content = \"\\n\".join(lines)\n",
        "        samples.append(content)\n",
        "        labels.append(class_index)\n",
        "    class_index += 1\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of samples:\", len(samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-dphjZ1PMcCV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2pmvE6gMcxT"
      },
      "source": [
        "# Mezclando los datos para separarlos en Traning y Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DYX7x-k_lCgZ"
      },
      "outputs": [],
      "source": [
        "# Shuffle the data\n",
        "seed = 1337\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(samples)\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Extract a training & validation split\n",
        "validation_split = 0.2\n",
        "num_validation_samples = int(validation_split * len(samples))\n",
        "train_samples = samples[:-num_validation_samples]\n",
        "val_samples = samples[-num_validation_samples:]\n",
        "train_labels = labels[:-num_validation_samples]\n",
        "val_labels = labels[-num_validation_samples:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMrE0T4wMj09"
      },
      "source": [
        "¿Por qué mezclamos los datos antes de separarlos en entrenamiento y validación?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OHjeoH7Rp1jm"
      },
      "outputs": [],
      "source": [
        "#Tu respuesta aqui\n",
        "\n",
        "#Es necesario dividirlas para asegurarnos que las clases estén correctamente balanceadas tanto en la distribución que se usará para entrenamiento como para la de validación. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mBO_XEZAp1vK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QJ2QGCeGp1yF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IktOtKfpNx8E"
      },
      "source": [
        "# Tokenización de las palabras con TextVectorization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QjHgQPX8lCjO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
        "vectorizer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIWC37s5smZ4",
        "outputId": "995f9dbe-81f5-4835-f9ab-fd1c3ae1733e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'to', 'of']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "vectorizer.get_vocabulary()[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vit8TPqTvmwS",
        "outputId": "0f34e6be-93cd-4dcf-f307-430b9519034c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(vectorizer.get_vocabulary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NmftvUeeF65s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yttb9K8vssAz"
      },
      "source": [
        "Pregunta. En la construcción del vocabulario hemos limitado el número de tokens ¿Podrías indicar el número de token diferentes o tamaño del vocabulario sin limitar el número de tokens? Es decir, ¿Cuántas palabras diferentes existen en los documentos procesados como instancias?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SZIgywwYquDE"
      },
      "outputs": [],
      "source": [
        "#Tu código aqui\n",
        "\n",
        "#Para responder a la pregunta hay que hacer uso nuevo del método TextVectorization pero en esta ocasión no limitarle el número de tokens, es decir, eliminando donde indica max_tokens=20.000. A continuación se deja el código para resolver esta pregunta.\n",
        "#vectorizer2 = TextVectorization(output_sequence_length=200)\n",
        "#text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
        "#vectorizer2.adapt(text_ds)\n",
        "#len(vectorizer2.get_vocabulary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LzE1Ae6pF7yo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O-FXA9wPVkg"
      },
      "source": [
        "# Viendo la salida de Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rseIF0fLmyJ0",
        "outputId": "e7de9936-fa58-42b0-a7ae-d21804b1bd5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2, 3503, 1819,   15,    2, 6631])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "output = vectorizer([[\"the cat sat on the mat\"]])\n",
        "output.numpy()[0, :6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsr4AQtBFArV",
        "outputId": "b3dd781a-cbe9-4ba6-bf2b-daf41aa6291e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 200), dtype=int64, numpy=\n",
              "array([[   2, 3503, 1819,   15,    2, 6631,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SL5ag8UamzwL"
      },
      "outputs": [],
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08v8SKcsn3lf",
        "outputId": "afeb5373-5593-4385-d4a2-5a3963a4eb63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3503, 1819, 15, 2, 6631]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "[word_index[w] for w in test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Zau4Xlu5F_IB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWxZ5Z0sF_d8"
      },
      "source": [
        "Pregunta. La salida de vectorizer() para codificar los tokens [\"El\", \"gato\", \"está\", \"sobre\", \"el\", \"tejado\"] es la siguiente [1, 121, 405, 1, 45, 4561]. Si cada uno de los valores indica el índice en el que se encuentra cada palabra en el array creado para codificarla. ¿Podría ser correcta esta salida?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "W9Xfg6MJFMZF"
      },
      "outputs": [],
      "source": [
        "#Tu respuesta aqui\n",
        "\n",
        "#No es correcta la salida porque como se puede ver el token “El” ha sido codificado con un 2 (es decir, con todo ceros salvo la un 1 en la posición 2) y el token “sobre” también ha sido codificado con un 2, es decir, exactamente igual que el token “El”. Cada “token” debe tener una codificación diferente. Para más error, se puede ver que “El” posteriormente tiene asignada otra codificación diferente 45 (es decir, un vector con todo ceros salvo un uno en la posición 45. Esto no es posible. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yXlhllZAFMb7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YWalbsjUKTLy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eBhadrvOTNZ"
      },
      "source": [
        "# Tokenización de los datos de entrenamiento y validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "W26LUr2dKTOj"
      },
      "outputs": [],
      "source": [
        "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
        "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PNu69FgnOarK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ur0mBBesOa3s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3QVIb84Olda"
      },
      "source": [
        "#Creación del modelo con el embedding GloVe y redes neuronales convolucionales\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkzAfY0v72EM",
        "outputId": "422f8083-1b02-416d-90c0-1dec1ecb5639"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-06 18:15:23--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-02-06 18:15:23--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-02-06 18:15:24--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.10MB/s    in 2m 39s  \n",
            "\n",
            "2023-02-06 18:18:04 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "m-bSZXrqoAEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "295e4d91-0c84-464e-9bd9-06100ae368df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "path_to_glove_file = os.path.join(\n",
        "    os.path.expanduser(\"~\"), \".keras/datasets/glove.6B.100d.txt\"\n",
        ")\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos la capa Embedding de keras. Se trata de una matrux numpy donde el valor de cada posición corresponde con el vector pre-entreneado del vocabulario tras el vectorizer"
      ],
      "metadata": {
        "id": "UdKbdVax2e_l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "R2Tov2B6oBuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8942c705-789e-46e9-db83-91fbe913f40a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 17954 words (2046 misses)\n"
          ]
        }
      ],
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el embedding como una capa keras. Al poner trainable=False nos quedamos con los valores del modelo pre-entrenado, es decir, no actualizamos estos valores a lo largo del entrenamiento"
      ],
      "metadata": {
        "id": "akWSH27b21AY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "w40nHAAPoD0J"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VTDrdvQyOlda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28748a1a-c991-44a8-d867-ce73d00e8649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 100)         2000200   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, None, 128)         64128     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, None, 128)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, None, 128)        0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 128)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20)                2580      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,247,516\n",
            "Trainable params: 247,316\n",
            "Non-trainable params: 2,000,200\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "modelEmbeddingGloveConvolucionales = keras.Model(int_sequences_input, preds)\n",
        "modelEmbeddingGloveConvolucionales.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qD_hqn2sOlda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe3830a-de71-49d8-9ac7-d0f5465adc6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "125/125 [==============================] - 11s 13ms/step - loss: 2.7262 - acc: 0.1245 - val_loss: 2.1654 - val_acc: 0.2668\n",
            "Epoch 2/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 1.9969 - acc: 0.3134 - val_loss: 1.6042 - val_acc: 0.4614\n",
            "Epoch 3/20\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.5341 - acc: 0.4734 - val_loss: 1.3072 - val_acc: 0.5454\n",
            "Epoch 4/20\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 1.2883 - acc: 0.5586 - val_loss: 1.1441 - val_acc: 0.6012\n",
            "Epoch 5/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 1.1110 - acc: 0.6198 - val_loss: 1.2384 - val_acc: 0.5806\n",
            "Epoch 6/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.9792 - acc: 0.6641 - val_loss: 1.1034 - val_acc: 0.6379\n",
            "Epoch 7/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.8691 - acc: 0.7017 - val_loss: 1.0795 - val_acc: 0.6422\n",
            "Epoch 8/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.7492 - acc: 0.7395 - val_loss: 1.0980 - val_acc: 0.6639\n",
            "Epoch 9/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.6718 - acc: 0.7685 - val_loss: 1.2902 - val_acc: 0.6304\n",
            "Epoch 10/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.5772 - acc: 0.7990 - val_loss: 1.0220 - val_acc: 0.6829\n",
            "Epoch 11/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.5105 - acc: 0.8203 - val_loss: 1.1918 - val_acc: 0.6552\n",
            "Epoch 12/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.4386 - acc: 0.8499 - val_loss: 1.2660 - val_acc: 0.6702\n",
            "Epoch 13/20\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3832 - acc: 0.8668 - val_loss: 1.0733 - val_acc: 0.7072\n",
            "Epoch 14/20\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.3356 - acc: 0.8840 - val_loss: 1.3353 - val_acc: 0.6627\n",
            "Epoch 15/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.3039 - acc: 0.8991 - val_loss: 1.8047 - val_acc: 0.6029\n",
            "Epoch 16/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.2717 - acc: 0.9084 - val_loss: 1.4404 - val_acc: 0.6727\n",
            "Epoch 17/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.2346 - acc: 0.9193 - val_loss: 1.2952 - val_acc: 0.7169\n",
            "Epoch 18/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.2150 - acc: 0.9265 - val_loss: 1.2870 - val_acc: 0.7039\n",
            "Epoch 19/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.2046 - acc: 0.9315 - val_loss: 1.3206 - val_acc: 0.7099\n",
            "Epoch 20/20\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.1857 - acc: 0.9376 - val_loss: 1.7414 - val_acc: 0.6449\n",
            "125/125 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "modelEmbeddingGloveConvolucionales.compile(    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
        "modelEmbeddingGloveConvolucionales.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))\n",
        "predictions = modelEmbeddingGloveConvolucionales.predict(x_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AT8KYdHaOh31"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4x_4eXJVrnX"
      },
      "source": [
        "#Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fgg7KnoioNYc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3b725972-b6a3-4c5e-e181-4d38a7c9c135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 185ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'comp.graphics'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorizer(string_input)\n",
        "preds = modelEmbeddingGloveConvolucionales(x)\n",
        "end_to_end_model = keras.Model(string_input, preds)\n",
        "\n",
        "probabilities = end_to_end_model.predict(\n",
        "    [[\"this message is about computer graphics and 3D modeling\"]]\n",
        ")\n",
        "\n",
        "class_names[np.argmax(probabilities[0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "R-EXfK6qoSAd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e7bb0be7-f47e-4797-a89b-29b7cdad0f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'talk.politics.mideast'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "probabilities = end_to_end_model.predict(\n",
        "    [[\"we are talking about politics\"]]\n",
        ")\n",
        "\n",
        "class_names[np.argmax(probabilities[0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QByfYDv4rGqv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7e2cad98-3b3a-4439-b63b-9e7d6e384cc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 30ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'soc.religion.christian'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "probabilities = end_to_end_model.predict(\n",
        "    [[\"we are talking about religion\"]]\n",
        ")\n",
        "\n",
        "class_names[np.argmax(probabilities[0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9ceed0nfrGtd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "MdD9NHIKXOdl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}